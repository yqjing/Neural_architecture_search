{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU0ypnFr9-Pg"
      },
      "source": [
        "# Mar 14 Meeting\n",
        "\n",
        "- File1: All kinds of components of a CNN, e.g. residual connection, batch norm,\n",
        "- File2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuYrY1n6_tvf"
      },
      "source": [
        "# Genetic CNN\n",
        "\n",
        "- we describe a way of representing the network structure by a fixed-length bi- nary string.\n",
        "- several genetic operations are defined, including selection, mutation and crossover, so that we can traverse the search space efficiently and find high-quality solutions.\n",
        "- ReLU and batch normalization are added after each convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_kx06ztv9pO8"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "# from gpu import get_gpu_status\n",
        "from torchsummary import summary\n",
        "import gc\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "from functools import partial\n",
        "import itertools\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JOy8HLwDuSvO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from ops import Identity, Sep_Conv, Conv, Stacked_conv, Pooling, Dil_Conv, Op\n",
        "\n",
        "class Block(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Block object that inherits the Op\n",
        "    \"\"\"\n",
        "    def __init__(self, num_actions, action_list, num_channels, strides):\n",
        "        super().__init__()\n",
        "        assert len(action_list) == num_actions, \"the length of the action list must be equal to the number of actions for the block\"\n",
        "        if strides == 2:\n",
        "            assert len(action_list) <= 3 and num_actions <= 3, \"the input block must have less than or equal to 3 layers\"\n",
        "        self.num_actions = num_actions\n",
        "        self.action_list = action_list\n",
        "        self.num_channels = num_channels\n",
        "        self.strides = strides\n",
        "        self.out_channels = num_channels\n",
        "        self.identity = True\n",
        "        self.build_block()\n",
        "\n",
        "\n",
        "    def build_block(self):\n",
        "        self.block = nn.ModuleList([])\n",
        "        action_ls_tmp = copy.deepcopy(self.action_list)\n",
        "        num_actions_tmp = copy.deepcopy(self.num_actions)\n",
        "\n",
        "        if self.action_list[0] == \"identity\":\n",
        "            self.identity = True\n",
        "        else:\n",
        "            self.identity = False\n",
        "\n",
        "        if self.identity == True:\n",
        "            if self.strides == 1:\n",
        "                self.skip_layer = self.str_2_action(self.num_channels, self.num_channels, 'identity', 1)\n",
        "                del action_ls_tmp[0]\n",
        "                num_actions_tmp -= 1\n",
        "            else:\n",
        "                self.skip_layer = self.str_2_action(self.num_channels, 64, 'identity', 2)\n",
        "                del action_ls_tmp[0]\n",
        "                num_actions_tmp -= 1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        for idx in range(num_actions_tmp):\n",
        "            action = action_ls_tmp[idx]\n",
        "            if self.strides == 1:\n",
        "                layer = self.str_2_action(self.num_channels, self.num_channels, action, 1)\n",
        "                self.block.append(layer)\n",
        "            else:\n",
        "                if idx == 0:\n",
        "                    layer = self.str_2_action(3, 32, action, 2)\n",
        "                elif idx == 1:\n",
        "                    layer = self.str_2_action(32, 64, action, 2)\n",
        "                elif idx == 2:\n",
        "                    layer = self.str_2_action(64, 128, action, 2)\n",
        "                self.block.append(layer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for op in self.block:\n",
        "            x = op(x)\n",
        "        if self.identity == True:\n",
        "            skip = self.skip_layer(inputs)\n",
        "            x = nn.functional.relu(x + skip)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def str_2_action(self, in_channels, num_channels, action, strides):\n",
        "\n",
        "        if action == \"3*3 dconv\":\n",
        "            x = Sep_Conv(in_channels, num_channels, 3, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"5*5 dconv\":\n",
        "            x = Sep_Conv(in_channels, num_channels, 5, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"3*3 conv\":\n",
        "            x = Conv(num_channels, 3, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"5*5 conv\":\n",
        "            x = Conv(num_channels, 5, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"1*7-7*1 conv\":\n",
        "            x = Stacked_conv([num_channels, num_channels], [strides, strides])\n",
        "            return x\n",
        "\n",
        "        if action == \"3*3 dil conv\":\n",
        "            x = Dil_Conv(in_channels, num_channels, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"identity\":\n",
        "            x = Identity(num_channels, strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"3*3 maxpool\":\n",
        "            x = Pooling(in_channels, \"max\", strides)\n",
        "            return x\n",
        "\n",
        "        if action == \"3*3 avgpool\":\n",
        "            x = Pooling(in_channels, \"average\", strides)\n",
        "            return x\n",
        "\n",
        "\n",
        "# # test\n",
        "# action_list = [\"identity\", \"3*3 avgpool\", \"3*3 avgpool\", \"1*7-7*1 conv\"]\n",
        "# b_1 = Block(4, action_list, 128*2, 1)\n",
        "# x1 = torch.randn(32, 128*2, 32, 32)\n",
        "# y1 = b_1(x1)\n",
        "\n",
        "# print(summary(b_1, (128*2, 32, 32)))\n",
        "# print(y1.shape)\n",
        "\n",
        "# action_list = [\"identity\", \"5*5 dconv\", \"3*3 dconv\"]\n",
        "# b_2 = Block(3, action_list, 3, 2)\n",
        "# x2 = torch.randn(32, 3, 32, 32)\n",
        "# y2 = b_2(x2)\n",
        "\n",
        "# print(summary(b_2, (3, 32, 32)))\n",
        "# print(y2.shape)\n",
        "\n",
        "\n",
        "class Cell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Cell that builds on the Op object. Cell is composed of Blocks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    cell_encoding : List[List]\n",
        "         [filter_size: int, num_blocks: int, action_list: [\"identity\", \"3*3 avgpool\", \"1*7-7*1 conv\"]]\n",
        "\n",
        "    cell_idx : int\n",
        "        idx of the cell\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cell_idx, cell_encoding, strides=1):\n",
        "        super().__init__()\n",
        "        self.action_list = cell_encoding[2]\n",
        "        self.num_blocks = cell_encoding[1]\n",
        "        self.strides = strides\n",
        "        self.num_channels = cell_encoding[0]\n",
        "        self.cell_idx = cell_idx\n",
        "        self.build_cell()\n",
        "\n",
        "    def build_cell(self):\n",
        "        self.cell = nn.ModuleList([])\n",
        "        self.first_layer = nn.LazyConv2d(self.num_channels, kernel_size=1, stride=1)\n",
        "        self.cell.append(self.first_layer)\n",
        "        for _ in range(self.num_blocks):\n",
        "            block = Block(len(self.action_list), self.action_list, self.num_channels, 1)\n",
        "            self.cell.append(block)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for block_op in self.cell:\n",
        "                x = block_op(x)\n",
        "        return x\n",
        "\n",
        "    def cell_summary(self):\n",
        "         print(f\"For Cell {self.cell_idx} | the Resolution of the image is 32 * 32 | the channel size is {self.num_channels} | the number of blocks are {self.num_blocks}.\")\n",
        "         print(\"The summary of the block is\")\n",
        "         block_fake = copy.deepcopy(self.cell[1])\n",
        "         print(summary(block_fake, (int(self.num_channels), 32, 32)))\n",
        "\n",
        "# # testing\n",
        "# ed = [300, 3, [\"identity\", \"3*3 avgpool\", \"1*7-7*1 conv\", \"5*5 dconv\"]]\n",
        "# c = Cell(3, ed)\n",
        "# x = torch.randn(32, 200, 32, 32)\n",
        "# y = c(x)\n",
        "# print(c.cell_summary())\n",
        "# print(y.shape)\n",
        "\n",
        "class Cell_input(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Cell that builds on the Op object. Cell is composed of Blocks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    cell_encoding : List[List]\n",
        "        [filter_size: int, num_blocks: int, action_list: [\"identity\", \"3*3 avgpool\", \"1*7-7*1 conv\"]]\n",
        "\n",
        "    cell_idx : int\n",
        "        idx of the cell\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cell_idx, cell_encoding, strides=2):\n",
        "        super().__init__()\n",
        "        self.action_list = cell_encoding[2]\n",
        "        self.num_blocks = cell_encoding[1]\n",
        "        self.strides = strides\n",
        "        self.num_channels = cell_encoding[0]\n",
        "        self.cell_idx = cell_idx\n",
        "        self.build_cell()\n",
        "\n",
        "    def build_cell(self):\n",
        "        self.cell = nn.ModuleList([])\n",
        "        for _ in range(self.num_blocks):\n",
        "            block = Block(len(self.action_list), self.action_list, self.num_channels, 2)\n",
        "            self.cell.append(block)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for block_op in self.cell:\n",
        "                x = block_op(x)\n",
        "        return x\n",
        "\n",
        "    def cell_summary(self):\n",
        "         print(f\"For Cell {self.cell_idx} | the Resolution of the image is 32 * 32 | the channel size is 3 -> 32 -> 64 | the number of blocks are {self.num_blocks}.\")\n",
        "         print(\"The summary of the block is\")\n",
        "         block_fake = copy.deepcopy(self.cell[0])\n",
        "         print(summary(block_fake, (3, 32, 32)))\n",
        "\n",
        "# # testing\n",
        "# ed = [np.inf, 1, [\"identity\", \"5*5 dconv\", \"3*3 conv\"]]\n",
        "# c = Cell_input(0, ed)\n",
        "# x = torch.randn(32, 3, 32, 32)\n",
        "# y = c(x)\n",
        "# print(y.shape)\n",
        "# c.cell_summary()\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Net object that inherits the Op that is the next level of Cell\n",
        "    \"\"\"\n",
        "    def __init__(self, net_encoding):\n",
        "        assert len(net_encoding) == 5, \"the number of cell in an individual must be 5\"\n",
        "        super().__init__()\n",
        "        self.net_ed = net_encoding\n",
        "        self.post_process_layer = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                                nn.Flatten(),\n",
        "                                                nn.LazyLinear(10),\n",
        "                                                nn.Softmax(dim=1))\n",
        "        self.build_net()\n",
        "\n",
        "    def build_net(self):\n",
        "        self.net = nn.ModuleList([])\n",
        "        cell_0 = Cell_input(0, self.net_ed[0])\n",
        "        cell_1 = Cell(1, self.net_ed[1])\n",
        "        cell_2 = Cell(2, self.net_ed[2])\n",
        "        cell_3 = Cell(3, self.net_ed[3])\n",
        "        cell_4 = Cell(4, self.net_ed[4])\n",
        "\n",
        "        self.net.append(cell_0)\n",
        "        self.net.append(cell_1)\n",
        "        self.net.append(cell_2)\n",
        "        self.net.append(cell_3)\n",
        "        self.net.append(cell_4)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for cell in self.net:\n",
        "            x = cell(x)\n",
        "        output = self.post_process_layer(x)\n",
        "        return output\n",
        "\n",
        "    def net_summary(self):\n",
        "        for cell in self.net:\n",
        "            cell.cell_summary()\n",
        "            print(\"\\n\")\n",
        "        print(\"Plus the post processing layer.\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6BpolVrhuTRG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Net object that inherits the Op that is the next level of Cell\n",
        "    \"\"\"\n",
        "    def __init__(self, net_encoding = None, learning_rate = 0.001, device='cpu'):\n",
        "        assert len(net_encoding) == 5, \"the number of cell in an individual must be 5\"\n",
        "        super().__init__()\n",
        "        if net_encoding == None:\n",
        "            self.net_ed = full_ed_generator(0.5)\n",
        "        else:\n",
        "            self.net_ed = net_encoding\n",
        "\n",
        "        self.post_process_layer = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                                nn.Flatten(),\n",
        "                                                nn.LazyLinear(10),\n",
        "                                                nn.Softmax(dim=1))\n",
        "        self.build_net()\n",
        "        self.performance_history=[]\n",
        "        self.learning_rate = learning_rate\n",
        "        self.device = device\n",
        "        self.assemble_model()\n",
        "\n",
        "    def assemble_model(self):\n",
        "        \"\"\"Assembles the neural network model.\"\"\"\n",
        "        self.model = nn.Sequential(*self.net, self.post_process_layer).to(self.device)\n",
        "\n",
        "    def update_performance(self, new_score):\n",
        "        self.performance_history.append(new_score)\n",
        "\n",
        "    def build_net(self):\n",
        "        self.net = nn.ModuleList([])\n",
        "        cell_0 = Cell_input(0, self.net_ed[0])\n",
        "        cell_1 = Cell(1, self.net_ed[1])\n",
        "        cell_2 = Cell(2, self.net_ed[2])\n",
        "        cell_3 = Cell(3, self.net_ed[3])\n",
        "        cell_4 = Cell(4, self.net_ed[4])\n",
        "\n",
        "        self.net.append(cell_0)\n",
        "        self.net.append(cell_1)\n",
        "        self.net.append(cell_2)\n",
        "        self.net.append(cell_3)\n",
        "        self.net.append(cell_4)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for cell in self.net:\n",
        "            x = cell(x)\n",
        "        output = self.post_process_layer(x)\n",
        "        return output\n",
        "\n",
        "    def net_summary(self):\n",
        "        for cell in self.net:\n",
        "            cell.cell_summary()\n",
        "            print(\"\\n\")\n",
        "        print(\"Plus the post processing layer.\\n\")\n",
        "\n",
        "    @property\n",
        "    def average_performance(self):\n",
        "        \"\"\"Calculate the average performance of the node.\"\"\"\n",
        "        if not self.performance_history:\n",
        "            return 0\n",
        "        return sum(self.performance_history) / len(self.performance_history)\n",
        "\n",
        "    def train(self, train_data, epochs=20, device='cpu'):\n",
        "        self.model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, labels in DataLoader(train_data, batch_size=64, shuffle=True):\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def evaluate_node(self, validation_data, device='cpu'):\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in DataLoader(validation_data, batch_size=64, shuffle=False):\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        self.performance_history.append(accuracy)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WLvou0oMuUw0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Op(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Op object, basic object\n",
        "    Each of them operates on a single tensor\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "\n",
        "    forward : method\n",
        "        Parameters\n",
        "        ---------\n",
        "\n",
        "        input : tensor\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        x : tensor\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "\n",
        "    operation = Op()\n",
        "    inputs = torch.randn(32, 3, 32, 32)\n",
        "    output = Op(inputs)\n",
        "\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "\n",
        "    The Op only applies to CIFAR10 image format i.e. 4d tensor\n",
        "    with shape [batch_size, num_channel, 32, 32]\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "class Identity(Op):\n",
        "    \"\"\"\n",
        "    Identity operation, which is None\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    num_channels : int\n",
        "        channel size\n",
        "    strides : int\n",
        "        1, or 2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, strides):\n",
        "        super().__init__()\n",
        "\n",
        "        if strides == 2:\n",
        "            self.op = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides, padding=16)\n",
        "        else:\n",
        "            self.op = lambda x: x\n",
        "        self.out_channels = num_channels\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.op(inputs)\n",
        "\n",
        "\n",
        "class Sep_Conv(Op):\n",
        "    \"\"\"\n",
        "    Seperable convolution, (depthwise conv -> pointwise conv) -> batchnorm -> relu\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    in_channels : int\n",
        "        in_channel size of input\n",
        "    num_channels : int\n",
        "        channel size, num of filters\n",
        "    kernel : int\n",
        "        kernel size, {3, 5, 7}\n",
        "    strides : int\n",
        "        either 1 or 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_channels, kernel, strides):\n",
        "        super().__init__()\n",
        "        assert kernel in [3, 5, 7], \"kernel not in the range {3, 5, 7}\"\n",
        "\n",
        "        if kernel == 3:\n",
        "            if strides == 1:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=17)\n",
        "        elif kernel == 5:\n",
        "            if strides == 1:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=18)\n",
        "        elif kernel == 7:\n",
        "            if strides == 1:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, padding=19)\n",
        "\n",
        "        self.conv_p = nn.LazyConv2d(num_channels, kernel_size=1)\n",
        "        self.bn = nn.LazyBatchNorm2d()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.out_channels = num_channels\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv_d(inputs)\n",
        "        x = self.conv_p(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Conv(Op):\n",
        "    \"\"\"\n",
        "    Base convolution object\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    num_channels : int\n",
        "        output channels\n",
        "    kernel : int\n",
        "        kernel size, {3, 7}\n",
        "    strides : int\n",
        "        stride size, {1, 2}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, kernel, strides):\n",
        "        super().__init__()\n",
        "\n",
        "        if kernel == 3:\n",
        "            if strides == 1:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=17)\n",
        "        if kernel == 5:\n",
        "            if strides == 1:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=18)\n",
        "        elif kernel == (1, 7):\n",
        "            if strides == 1:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=(16, 19))\n",
        "        elif kernel == (7, 1):\n",
        "            if strides == 1:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=\"same\")\n",
        "            else:\n",
        "                self.conv = nn.LazyConv2d(num_channels, kernel_size=kernel, stride=strides, padding=(19, 16))\n",
        "        self.bn = nn.LazyBatchNorm2d()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.out_channels = num_channels\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Stacked_conv(Op):\n",
        "    \"\"\"\n",
        "    Stacked convolution of 1 * 7 followed by 7 * 1 convolution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    channel_list : list[int]\n",
        "        e.g. [64, 128]\n",
        "    stride_list : list[int]\n",
        "        e.g. [1, 2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, channel_list, stride_list, kernel_list=[(1, 7), (7, 1)]):\n",
        "        super().__init__()\n",
        "        assert kernel_list == [(1, 7), (7, 1)], \"kernel list must be [(1, 7), (7, 1)]\"\n",
        "        assert len(channel_list) == len(kernel_list) and len(kernel_list) == len(stride_list), \"List lengths must match\"\n",
        "        self.convs = nn.ModuleList([])\n",
        "        for _, (c, k, s) in enumerate(zip(channel_list, kernel_list, stride_list)):\n",
        "            convolution = Conv(c, k, s)\n",
        "            self.convs.append(convolution)\n",
        "        self.out_channels = channel_list[1]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for op in self.convs:\n",
        "            x = op(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Pooling(Op):\n",
        "    \"\"\"\n",
        "    Pooling operation, two variations\n",
        "    1. 3 by 3 average pooling\n",
        "    2. 3 by 3 max pooling\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        input channel number\n",
        "    type : str\n",
        "        \"max\" or \"average\"\n",
        "    strides : int\n",
        "        1 or 2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, type, strides, size = 3):\n",
        "        super().__init__()\n",
        "        assert size == 3, \"kernel size must be 3\"\n",
        "        self.strides = strides\n",
        "        if type == \"max\":\n",
        "            if strides == 1:\n",
        "                self.pool = nn.MaxPool2d(size, strides, padding = int(np.floor(size / 2)))\n",
        "            else:\n",
        "                self.pad = nn.ZeroPad2d(16)\n",
        "                self.pool = nn.MaxPool2d(size, strides, padding = 1)\n",
        "        elif type == \"average\":\n",
        "            if strides == 1:\n",
        "                self.pool = nn.AvgPool2d(size, strides, padding = int(np.floor(size / 2)))\n",
        "            else:\n",
        "                self.pad = nn.ZeroPad2d(16)\n",
        "                self.pool = nn.AvgPool2d(size, strides, padding = 1)\n",
        "        self.out_channels = in_channels\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.strides == 2:\n",
        "            x = self.pad(inputs)\n",
        "            x = self.pool(x)\n",
        "        else:\n",
        "            x = self.pool(inputs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Dil_Conv(Op):\n",
        "    \"\"\"\n",
        "    3 by 3Seperable dilated convolution, (depthwise conv -> pointwise conv) -> batchnorm -> relu\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    in_channels : int\n",
        "        in_channel size of input\n",
        "    num_channels : int\n",
        "        channel size, num of filters\n",
        "    kernel : int\n",
        "        kernel size, {3, 5, 7}\n",
        "    strides : int\n",
        "        either 1 or 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_channels, strides, kernel=3, dilation=2):\n",
        "        super().__init__()\n",
        "        assert kernel == 3, \"kernel not equal to 3\"\n",
        "\n",
        "        if strides == 1:\n",
        "            self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, dilation=dilation, padding=\"same\")\n",
        "        else:\n",
        "            self.conv_d = nn.Conv2d(in_channels, in_channels, kernel_size=kernel, groups = in_channels, stride=strides, dilation=dilation, padding=18)\n",
        "\n",
        "        self.conv_p = nn.LazyConv2d(num_channels, kernel_size=1)\n",
        "        self.bn = nn.LazyBatchNorm2d()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.out_channels = num_channels\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv_d(inputs)\n",
        "        x = self.conv_p(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8l3iwq08uXC1"
      },
      "outputs": [],
      "source": [
        "\n",
        "kernel_sizes = [3, 5, 7]\n",
        "strides = [1, 2]\n",
        "in_channels_options = [32, 64]\n",
        "num_channels_options = [32, 64, 128]\n",
        "\n",
        "conv_combinations = [\n",
        "    partial(Conv, num_channels=num_channels, kernel=kernel, strides=stride)\n",
        "    for num_channels, kernel, stride in itertools.product(num_channels_options, kernel_sizes, strides)\n",
        "]\n",
        "\n",
        "sep_conv_combinations = [\n",
        "    partial(Sep_Conv, in_channels=in_channels, num_channels=num_channels, kernel=kernel, strides=stride)\n",
        "    for in_channels, num_channels, kernel, stride in itertools.product(in_channels_options, num_channels_options, kernel_sizes, strides)\n",
        "]\n",
        "\n",
        "available_ops = conv_combinations + sep_conv_combinations\n",
        "\n",
        "action_ls_full = [\"identity\", \"3*3 dconv\",  \"5*5 dconv\", \"3*3 conv\", \"5*5 conv\", \"1*7-7*1 conv\", \"3*3 dil conv\", \"3*3 maxpool\", \"3*3 avgpool\"]\n",
        "action_ls_input = [\"identity\", \"3*3 dconv\",  \"5*5 dconv\", \"3*3 conv\", \"5*5 conv\"]\n",
        "\n",
        "\n",
        "\n",
        "def load_cifar10_data():\n",
        "    \"\"\"Load and preprocess CIFAR-10 data.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 data\n",
        "    train_data_full = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    train_size = int(0.8 * len(train_data_full))\n",
        "    validation_size = len(train_data_full) - train_size\n",
        "    train_data, validation_data = torch.utils.data.random_split(train_data_full, [train_size, validation_size])\n",
        "\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "def input_ed_generator(random_pre):\n",
        "    ed_input = []\n",
        "    action_ls = copy.deepcopy(action_ls_input)\n",
        "    random_p = random.random()\n",
        "    if random_p >= random_pre:\n",
        "        ed_input.append('identity')\n",
        "        del action_ls[0]\n",
        "    else:\n",
        "        del action_ls[0]\n",
        "    len_remain = 3 - len(ed_input)\n",
        "    ed_input = ed_input + [random.choice(action_ls) for _ in range(len_remain)]\n",
        "    return ed_input\n",
        "\n",
        "def normal_ed_generator(random_pre):\n",
        "    ed_input = []\n",
        "    action_ls = copy.deepcopy(action_ls_full)\n",
        "    random_p = random.random()\n",
        "    if random_p >= random_pre:\n",
        "        ed_input.append('identity')\n",
        "        del action_ls[0]\n",
        "    num_ls = [2, 3, 4]\n",
        "    num_actions = random.choice(num_ls)\n",
        "    len_remain = num_actions - len(ed_input)\n",
        "    ed_input = ed_input + [random.choice(action_ls) for _ in range(len_remain)]\n",
        "    return ed_input\n",
        "\n",
        "def full_ed_generator(random_pre):\n",
        "    \"\"\"\n",
        "    Net encoding generator\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    random_pre : float\n",
        "        [0, 1],\n",
        "        if randomly chosen probability random_p > random_pre,\n",
        "        assign \"identity\" operation to the first position of the action list\n",
        "\n",
        "    Variables in the encoding of a cell\n",
        "    -----------------------------------\n",
        "\n",
        "    [num_channels: int, num_blocks: int, action_list: [\"identity\", \"3*3 avgpool\", \"1*7-7*1 conv\"]]\n",
        "\n",
        "    num_channel : int\n",
        "        randomly chosen from the list [24, 40, 64, 80, 128, 256]\n",
        "    action_list : List[str]\n",
        "        a list of string, e.g. [\"identity\", \"3*3 avgpool\", \"1*7-7*1 conv\"],\n",
        "        randomly initialized.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    net_ed = []\n",
        "    channel_list = [24, 40, 64, 80, 128, 256]\n",
        "    ed_cell_0 = [np.inf, 1, input_ed_generator(random_pre)]\n",
        "    ed_cell_1 = [random.choice(channel_list), 1, normal_ed_generator(random_pre)]\n",
        "    ed_cell_2 = [random.choice(channel_list), 2, normal_ed_generator(random_pre)]\n",
        "    ed_cell_3 = [random.choice(channel_list), 3, normal_ed_generator(random_pre)]\n",
        "    ed_cell_4 = [random.choice(channel_list), 4, normal_ed_generator(random_pre)]\n",
        "\n",
        "    net_ed.append(ed_cell_0)\n",
        "    net_ed.append(ed_cell_1)\n",
        "    net_ed.append(ed_cell_2)\n",
        "    net_ed.append(ed_cell_3)\n",
        "    net_ed.append(ed_cell_4)\n",
        "\n",
        "    return net_ed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "C9zOYC6eueGs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def initialize_population(size=10, device = 'cpu'):\n",
        "    \"\"\"Initialize a population of networks.\"\"\"\n",
        "    population = [Network(net_encoding=full_ed_generator(0.5), device = device) for _ in range(size)]\n",
        "    return population\n",
        "\n",
        "def evaluate_fitness(node, train_data, validation_data, epoch = 20, device ='cpu', save=True):\n",
        "    \"\"\"Evaluate the fitness of a node by training and then measuring performance.\"\"\"\n",
        "    node.train(train_data, epochs=epoch, device = device)\n",
        "    performance_score = node.evaluate_node(validation_data, device = device)\n",
        "    if save:\n",
        "      node.update_performance(performance_score)\n",
        "    return performance_score\n",
        "\n",
        "\n",
        "def select_population(population, to_keep=5):\n",
        "    \"\"\"Select the best-performing nodes based on average performance.\"\"\"\n",
        "    sorted_population = sorted(population, key=lambda x: x.average_performance, reverse=True)\n",
        "    return sorted_population[:to_keep]\n",
        "\n",
        "def weighted_selection(nodes, number_of_parents=2):\n",
        "    \"\"\"Selects parents based on their average performance.\"\"\"\n",
        "    total_performance = sum(node.average_performance for node in nodes)\n",
        "    if total_performance == 0:\n",
        "        weights = [1/len(nodes)] * len(nodes)\n",
        "    else:\n",
        "        weights = [node.average_performance / total_performance for node in nodes]\n",
        "\n",
        "    selected_parents = random.choices(nodes, weights=weights, k=number_of_parents)\n",
        "    return selected_parents\n",
        "\n",
        "def crossover_and_mutate(parents, available_ops, population_size=15, to_keep=5, device = 'cpu'):\n",
        "    \"\"\"Generate a new population, ensuring the best 'to_keep' nodes are included.\"\"\"\n",
        "    new_population = parents\n",
        "    while len(new_population) < population_size:\n",
        "        parent1, parent2 = weighted_selection(parents, number_of_parents=2)\n",
        "        child = create_new_child(parent1, parent2, device = device)\n",
        "        new_population.append(child)\n",
        "\n",
        "    return new_population\n",
        "\n",
        "def create_new_child(parent1, parent2, base_mutation_rate=0.05, mutation_increase=0.05, device = 'cpu', random_pre=0.2, channel_list = [24, 40, 64, 80, 128, 256]):\n",
        "    child_net_ed = []\n",
        "    num_cells = len(parent1.net_ed)\n",
        "\n",
        "    for i in range(num_cells):\n",
        "        if random.random() < 0.5:\n",
        "            child_net_ed.append(copy.deepcopy(parent1.net_ed[i]))\n",
        "        else:\n",
        "            child_net_ed.append(copy.deepcopy(parent2.net_ed[i]))\n",
        "        adj = float(mutation_increase) * float(i / (num_cells - 1))\n",
        "        adjusted_mutation_rate = float(base_mutation_rate) + adj\n",
        "\n",
        "        if random.random() < adjusted_mutation_rate:\n",
        "            if i == 0:\n",
        "                ed_cell = [np.inf, 1, input_ed_generator(random_pre)]\n",
        "            else:\n",
        "                ed_cell = [random.choice(channel_list), 1, normal_ed_generator(random_pre)]\n",
        "            child_net_ed[i] = ed_cell\n",
        "\n",
        "    child_net = Network(net_encoding=child_net_ed, learning_rate=random.choice([parent1.learning_rate, parent2.learning_rate]), device = device)\n",
        "    return child_net\n",
        "\n",
        "def select_data_subsets(train_data, subset_size=20):\n",
        "    \"\"\"\n",
        "    Selects random subsets from the training data for training and validation.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data: The dataset from which to select subsets.\n",
        "    - subset_size (int): The size of the subsets to select for both training and validation.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the training and validation subsets.\n",
        "    \"\"\"\n",
        "    indices = np.random.choice(len(train_data), 2 * subset_size, replace=False)\n",
        "    train_indices = indices[:subset_size]\n",
        "    val_indices = indices[subset_size:]\n",
        "    train_subset = Subset(train_data, train_indices)\n",
        "    val_subset = Subset(train_data, val_indices)\n",
        "\n",
        "    return train_subset, val_subset\n",
        "\n",
        "def genetic_algorithm(train_data, validation_data, generations=100, population_size=10, to_keep=5, subset_size=120, train_epoches = 10, device = 'cpu'):\n",
        "    population = initialize_population(size=population_size, device = device)\n",
        "    for generation in tqdm(range(generations)):\n",
        "        print(f\"Generation {generation + 1}\")\n",
        "        train_subset, test_subset = select_data_subsets(train_data, subset_size=subset_size)\n",
        "\n",
        "        test_performances = []\n",
        "        i = 0\n",
        "        for node in tqdm(population):\n",
        "            performance_score = evaluate_fitness(node, train_subset, test_subset, train_epoches, device)\n",
        "            test_performances.append(performance_score)\n",
        "            #print(f\"training on node {i}, performance {performance_score}\")\n",
        "            i+=1\n",
        "\n",
        "        print(f\"Finish Generation {i}\")\n",
        "        avg_test_performance = sum(test_performances) / len(test_performances)\n",
        "        best_child_index = test_performances.index(max(test_performances))\n",
        "        best_child = population[best_child_index]\n",
        "        best_child_test_score = test_performances[best_child_index]\n",
        "\n",
        "        print(f\"Average performance on test subset: {avg_test_performance}\")\n",
        "        print(f\"Best child's performance on test subset: {best_child_test_score}\")\n",
        "        # print(f\"Best child's performance on validation subset{evaluate_fitness(best_child, train_data, validation_data, 10, device, save=False)}\")\n",
        "        print(f\"Best child's structure: {summary(best_child, (3, 32, 32))}\")\n",
        "\n",
        "        selected = select_population(population, to_keep=to_keep)\n",
        "        population = crossover_and_mutate(selected, available_ops=available_ops, population_size=population_size, to_keep=to_keep, device = device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "l6xIY1t0uf1c",
        "outputId": "54abb98c-a086-4e9a-db38-863af3cc8e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|â–ˆ         | 1/10 [00:09<01:28,  9.85s/it]\n",
            "  0%|          | 0/100 [00:09<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-e8bebdded83a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Using device: {device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenetic_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-6cd37c67b383>\u001b[0m in \u001b[0;36mgenetic_algorithm\u001b[0;34m(train_data, validation_data, generations, population_size, to_keep, subset_size, train_epoches, device)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mperformance_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mtest_performances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m#print(f\"training on node {i}, performance {performance_score}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-6cd37c67b383>\u001b[0m in \u001b[0;36mevaluate_fitness\u001b[0;34m(node, train_data, validation_data, epoch, device, save)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Evaluate the fitness of a node by training and then measuring performance.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mperformance_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-51aab2f2827f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, epochs, device)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "train_data, validation_data, _ = load_cifar10_data()\n",
        "genetic_algorithm(train_data, validation_data, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyLzgB1r01Ni"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
